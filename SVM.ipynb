{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeffrey Bradley, Taylor Tucker, Virginia Weston\n",
    "#### Test 2\n",
    "#### Prof. Watson\n",
    "#### 10/20/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import log_loss\n",
    "from mlxtend.plotting import scatterplotmatrix\n",
    "from mlxtend.plotting import heatmap\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix plot function used in previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n0        337.0        118.0                  4  4.5   4.5  9.65         1   \n1        324.0        107.0                  4  4.0   4.5  8.87         1   \n2        316.0        104.0                  3  3.0   3.5  8.00         1   \n3        322.0        110.0                  3  3.5   2.5  8.67         1   \n4        314.0        103.0                  2  2.0   3.0  8.21         0   \n..         ...          ...                ...  ...   ...   ...       ...   \n351      324.0        110.0                  3  3.5   3.5  9.04         1   \n352      325.0        107.0                  3  3.0   3.5  9.11         1   \n353      330.0        116.0                  4  5.0   4.5  9.45         1   \n354      312.0        103.0                  3  3.5   4.0  8.78         0   \n355      333.0        117.0                  4  5.0   4.0  9.66         1   \n\n     SES Percentage  Asian  african american  latinx  white  Chance of Admit  \n0                12      1                 0       0      0             0.92  \n1                11      0                 0       1      0             0.76  \n2                78      0                 0       1      0             0.72  \n3                77      0                 0       0      1             0.80  \n4                 1      0                 1       0      0             0.65  \n..              ...    ...               ...     ...    ...              ...  \n351              60      0                 0       0      1             0.82  \n352              30      0                 0       1      0             0.84  \n353              99      0                 0       0      1             0.91  \n354              12      0                 1       0      0             0.67  \n355               7      1                 0       0      0             0.95  \n\n[356 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>SES Percentage</th>\n      <th>Asian</th>\n      <th>african american</th>\n      <th>latinx</th>\n      <th>white</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>337.0</td>\n      <td>118.0</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>324.0</td>\n      <td>107.0</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>316.0</td>\n      <td>104.0</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>78</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>322.0</td>\n      <td>110.0</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>77</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>314.0</td>\n      <td>103.0</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>351</th>\n      <td>324.0</td>\n      <td>110.0</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>9.04</td>\n      <td>1</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.82</td>\n    </tr>\n    <tr>\n      <th>352</th>\n      <td>325.0</td>\n      <td>107.0</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>9.11</td>\n      <td>1</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.84</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>330.0</td>\n      <td>116.0</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>4.5</td>\n      <td>9.45</td>\n      <td>1</td>\n      <td>99</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.91</td>\n    </tr>\n    <tr>\n      <th>354</th>\n      <td>312.0</td>\n      <td>103.0</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>4.0</td>\n      <td>8.78</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.67</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>333.0</td>\n      <td>117.0</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>9.66</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.95</td>\n    </tr>\n  </tbody>\n</table>\n<p>356 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('CleanedData.csv')\n",
    "df.drop(columns = \"Unnamed: 0\", inplace = True)\n",
    "columns = df.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar to our decision trees method AND Logistic Regression method, SVMs are binary classification so we will convert the Chance of admit to ones and zeros based on a threshold set by the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "median_value = df[\"Chance of Admit\"].median()\n",
    "print(median_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the median chance of admission is 73%, I will use that as the threshold. Any chance of admission >= 0.73 will be\n",
    "set to one, and any student < 0.73 will be set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1.0\n",
      "1      1.0\n",
      "2      0.0\n",
      "3      1.0\n",
      "4      0.0\n",
      "      ... \n",
      "351    1.0\n",
      "352    1.0\n",
      "353    1.0\n",
      "354    0.0\n",
      "355    1.0\n",
      "Name: Chance of Admit, Length: 356, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df[\"Chance of Admit\"])):\n",
    "    if df.loc[i, \"Chance of Admit\"] >= 0.73:\n",
    "        df.loc[i, \"Chance of Admit\"] = 1\n",
    "    else:\n",
    "        df.loc[i, \"Chance of Admit\"] = 0\n",
    "print(df[\"Chance of Admit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n0        337.0        118.0                  4  4.5   4.5  9.65         1   \n1        324.0        107.0                  4  4.0   4.5  8.87         1   \n2        316.0        104.0                  3  3.0   3.5  8.00         1   \n3        322.0        110.0                  3  3.5   2.5  8.67         1   \n4        314.0        103.0                  2  2.0   3.0  8.21         0   \n..         ...          ...                ...  ...   ...   ...       ...   \n351      324.0        110.0                  3  3.5   3.5  9.04         1   \n352      325.0        107.0                  3  3.0   3.5  9.11         1   \n353      330.0        116.0                  4  5.0   4.5  9.45         1   \n354      312.0        103.0                  3  3.5   4.0  8.78         0   \n355      333.0        117.0                  4  5.0   4.0  9.66         1   \n\n     SES Percentage  Asian  african american  latinx  white  \n0                12      1                 0       0      0  \n1                11      0                 0       1      0  \n2                78      0                 0       1      0  \n3                77      0                 0       0      1  \n4                 1      0                 1       0      0  \n..              ...    ...               ...     ...    ...  \n351              60      0                 0       0      1  \n352              30      0                 0       1      0  \n353              99      0                 0       0      1  \n354              12      0                 1       0      0  \n355               7      1                 0       0      0  \n\n[356 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>SES Percentage</th>\n      <th>Asian</th>\n      <th>african american</th>\n      <th>latinx</th>\n      <th>white</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>337.0</td>\n      <td>118.0</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>324.0</td>\n      <td>107.0</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>316.0</td>\n      <td>104.0</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>78</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>322.0</td>\n      <td>110.0</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>77</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>314.0</td>\n      <td>103.0</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>351</th>\n      <td>324.0</td>\n      <td>110.0</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>9.04</td>\n      <td>1</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>352</th>\n      <td>325.0</td>\n      <td>107.0</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>9.11</td>\n      <td>1</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>330.0</td>\n      <td>116.0</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>4.5</td>\n      <td>9.45</td>\n      <td>1</td>\n      <td>99</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>354</th>\n      <td>312.0</td>\n      <td>103.0</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>4.0</td>\n      <td>8.78</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>333.0</td>\n      <td>117.0</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>9.66</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>356 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[:,:12]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Chance of Admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in Logistic Regression, Robust Scaler improved accuracy the most, so we will be using it here aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_RS = preprocessing.RobustScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "xRS_train, xRS_test, yRS_train, yRS_test = train_test_split(x_RS, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "running a grid search for SVM classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.91      0.86        53\n",
      "         1.0       0.90      0.81      0.85        54\n",
      "\n",
      "    accuracy                           0.86       107\n",
      "   macro avg       0.86      0.86      0.86       107\n",
      "weighted avg       0.86      0.86      0.86       107\n",
      "\n",
      "linear 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.87        53\n",
      "         1.0       0.90      0.83      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "linear 1.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.91      0.86        53\n",
      "         1.0       0.90      0.81      0.85        54\n",
      "\n",
      "    accuracy                           0.86       107\n",
      "   macro avg       0.86      0.86      0.86       107\n",
      "weighted avg       0.86      0.86      0.86       107\n",
      "\n",
      "linear 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.87        53\n",
      "         1.0       0.90      0.83      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "linear 2.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.87        53\n",
      "         1.0       0.90      0.83      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "linear 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.87        53\n",
      "         1.0       0.90      0.83      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "poly 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.81      0.86        53\n",
      "         1.0       0.83      0.93      0.88        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "poly 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.87      0.88        53\n",
      "         1.0       0.87      0.89      0.88        54\n",
      "\n",
      "    accuracy                           0.88       107\n",
      "   macro avg       0.88      0.88      0.88       107\n",
      "weighted avg       0.88      0.88      0.88       107\n",
      "\n",
      "poly 1.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.87      0.88        53\n",
      "         1.0       0.87      0.89      0.88        54\n",
      "\n",
      "    accuracy                           0.88       107\n",
      "   macro avg       0.88      0.88      0.88       107\n",
      "weighted avg       0.88      0.88      0.88       107\n",
      "\n",
      "poly 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.87      0.88        53\n",
      "         1.0       0.87      0.89      0.88        54\n",
      "\n",
      "    accuracy                           0.88       107\n",
      "   macro avg       0.88      0.88      0.88       107\n",
      "weighted avg       0.88      0.88      0.88       107\n",
      "\n",
      "poly 2.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87        53\n",
      "         1.0       0.87      0.87      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "poly 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87        53\n",
      "         1.0       0.87      0.87      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n",
      "rbf 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.89      0.85        53\n",
      "         1.0       0.88      0.81      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "rbf 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.89      0.86        53\n",
      "         1.0       0.88      0.83      0.86        54\n",
      "\n",
      "    accuracy                           0.86       107\n",
      "   macro avg       0.86      0.86      0.86       107\n",
      "weighted avg       0.86      0.86      0.86       107\n",
      "\n",
      "rbf 1.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.87      0.85        53\n",
      "         1.0       0.87      0.83      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "rbf 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.87      0.85        53\n",
      "         1.0       0.87      0.83      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "rbf 2.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.87      0.85        53\n",
      "         1.0       0.87      0.83      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "rbf 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.87      0.85        53\n",
      "         1.0       0.87      0.83      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "sigmoid 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.89      0.85        53\n",
      "         1.0       0.88      0.80      0.83        54\n",
      "\n",
      "    accuracy                           0.84       107\n",
      "   macro avg       0.84      0.84      0.84       107\n",
      "weighted avg       0.84      0.84      0.84       107\n",
      "\n",
      "sigmoid 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.89      0.85        53\n",
      "         1.0       0.88      0.81      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "sigmoid 1.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.91      0.86        53\n",
      "         1.0       0.90      0.80      0.84        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.86      0.85      0.85       107\n",
      "\n",
      "sigmoid 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.91      0.86        53\n",
      "         1.0       0.90      0.80      0.84        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.86      0.85      0.85       107\n",
      "\n",
      "sigmoid 2.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.89      0.85        53\n",
      "         1.0       0.88      0.81      0.85        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.85      0.85      0.85       107\n",
      "\n",
      "sigmoid 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.91      0.86        53\n",
      "         1.0       0.90      0.80      0.84        54\n",
      "\n",
      "    accuracy                           0.85       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.86      0.85      0.85       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "cs = [0.5, 1, 1.5, 2, 2.5, 3]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for c in cs:\n",
    "        model = SVC(kernel=kernel, C=c)\n",
    "        model.fit(xRS_train, yRS_train)\n",
    "        y_pred = model.predict(xRS_test)\n",
    "\n",
    "        print(kernel, c)\n",
    "        print(classification_report(yRS_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This grid search shows that the linear kernel results in the highest accuracy of roughly 93%. The grid search also shows\n",
    "that we do not need to be worried about the C value because it doesn't affect the accuracy of the linear kernel predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n       1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n       0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n       0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n       1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n       0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n       1., 1., 1., 1., 0.])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "SVM.fit(xRS_train, yRS_train)\n",
    "yhatSVM = SVM.predict(xRS_test)\n",
    "yhatSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7627118644067796"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(yRS_test, yhatSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.87        53\n",
      "         1.0       0.90      0.83      0.87        54\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.87      0.87      0.87       107\n",
      "weighted avg       0.87      0.87      0.87       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(yRS_test, yhatSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[45  9]\n",
      " [ 5 48]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CM = confusion_matrix(yRS_test, yhatSVM, labels=[1,0])\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(CM, classes=['Chance of Admit = 1','Chance of Admit = 0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SVM appears to have done the best out of all the binary classifiers we used on this data set\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}